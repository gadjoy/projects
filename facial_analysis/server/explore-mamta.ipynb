{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download path below\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path or 0 for webcam)\n",
    "image = cv2.imread(\"sal.jpg\") \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Find landmarks for each detected face\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)c:\\Users\\hp\\Documents\\img22.jpg\n",
    "    print(len(landmarks.parts()))\n",
    "\n",
    "    # Draw circles on each landmark point\n",
    "    for n in range(0, 5):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        cv2.circle(image, (x, y), 4, (255, 0, 127), -1)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"sal-out.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download path below\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path or 0 for webcam)\n",
    "image = cv2.imread(\"img22.jpg\") \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Find landmarks for each detected face\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "\n",
    "    # Draw circles on each landmark point\n",
    "    for n in range(0, 68):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        cv2.circle(image, (x, y), 4, (255, 0, 127), -1)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"img22-out.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download path below\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path)\n",
    "image = cv2.imread(\"img11.jpg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Define rules for emotion recognition\n",
    "def recognize_emotion(landmarks):\n",
    "    # Example rule: If mouth curvature is upward and eyes are wide open, classify as \"happy\"\n",
    "    mouth_curvature = landmarks[54][1] - landmarks[48][1]\n",
    "    eye_openness = (landmarks[43][1] - landmarks[47][1]) + (landmarks[44][1] - landmarks[46][1])\n",
    "    if mouth_curvature > 10 and eye_openness > 10:\n",
    "        return \"happy\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Analyze emotions for each detected face\n",
    "for face in faces:\n",
    "    landmarks = [(p.x, p.y) for p in predictor(gray, face).parts()]\n",
    "    emotion = recognize_emotion(landmarks)\n",
    "    # Draw detected emotion on the image\n",
    "    cv2.putText(image, emotion, (face.left(), face.top() - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"emotion_analysis_output.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Draw circles on each landmark point\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m125\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     24\u001b[0m     y \u001b[38;5;241m=\u001b[39m landmarks\u001b[38;5;241m.\u001b[39mpart(n)\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m     25\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mcircle(image, (x, y), \u001b[38;5;241m4\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download path below\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path or 0 for webcam)\n",
    "image = cv2.imread(\"alia.jpg\") \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Find landmarks for each detected face\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "\n",
    "    # Draw circles on each landmark point\n",
    "    for n in range(0, 125):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"alia-out.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open shape_predictor_125_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m predictor_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape_predictor_125_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Download path below\u001b[39;00m\n\u001b[1;32m      7\u001b[0m detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[0;32m----> 8\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load an image (replace with your image path or 0 for webcam)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg11.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to open shape_predictor_125_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_125_face_landmarks.dat\"  # Download path below\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path or 0 for webcam)\n",
    "image = cv2.imread(\"img11.jpg\") \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Define facial expression regions (example)\n",
    "left_eye_points = list(range(36, 42))\n",
    "right_eye_points = list(range(42, 48))\n",
    "mouth_points = list(range(48, 68))\n",
    "\n",
    "# Function to classify facial expressions\n",
    "def classify_expression(landmarks):\n",
    "    expression_label = \"Sample Expression\"  # Replace with your actual classification result\n",
    "    \n",
    "    return expression_label\n",
    "    # Extract features from landmarks\n",
    "    # Implement your feature extraction logic here\n",
    "    \n",
    "    # Classify facial expression\n",
    "    # Implement your classification logic here\n",
    "    \n",
    "    return expression_label\n",
    "\n",
    "# Find landmarks for each detected face\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "    \n",
    "    # Classify facial expression\n",
    "    expression_label = classify_expression(landmarks)\n",
    "    \n",
    "    # Draw rectangles around the facial expression regions\n",
    "    # You can modify this part to suit your visualization needs\n",
    "    for point in left_eye_points + right_eye_points + mouth_points:\n",
    "        x = landmarks.part(point).x\n",
    "        y = landmarks.part(point).y\n",
    "        cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the classified facial expression\n",
    "    cv2.putText(image, expression_label, (face.left(), face.bottom()), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"output_img11.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download path below\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path)\n",
    "image = cv2.imread(\"alia.jpg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Define rules for emotion recognition\n",
    "def recognize_emotion(landmarks):\n",
    "    # Example rule: If mouth curvature is upward and eyes are wide open, classify as \"happy\"\n",
    "    mouth_curvature = landmarks[54][1] - landmarks[48][1]\n",
    "    eye_openness = (landmarks[43][1] - landmarks[47][1]) + (landmarks[44][1] - landmarks[46][1])\n",
    "    if mouth_curvature > 10 and eye_openness > 10:\n",
    "        return \"sad\"\n",
    "    else:\n",
    "        return \"happy\"\n",
    "\n",
    "# Analyze emotions for each detected face\n",
    "for face in faces:\n",
    "    landmarks = [(p.x, p.y) for p in predictor(gray, face).parts()]\n",
    "    emotion = recognize_emotion(landmarks)\n",
    "    # Draw detected emotion on the image\n",
    "    cv2.putText(image, emotion, (face.left(), face.top() - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"alia_output.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open shape_predictor_125_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m predictor_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape_predictor_125_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Updated path\u001b[39;00m\n\u001b[1;32m      7\u001b[0m detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[0;32m----> 8\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load an image (replace with your image path or 0 for webcam)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg11.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to open shape_predictor_125_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_125_face_landmarks.dat\"  # Updated path\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path or 0 for webcam)\n",
    "image = cv2.imread(\"img11.jpg\") \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Define facial expression regions (example)\n",
    "left_eye_points = list(range(36, 42))\n",
    "right_eye_points = list(range(42, 48))\n",
    "mouth_points = list(range(48, 68))\n",
    "\n",
    "# Function to classify facial expressions\n",
    "def classify_expression(landmarks):\n",
    "    expression_label = \"Sample Expression\"  # Replace with your actual classification result\n",
    "    \n",
    "    return expression_label\n",
    "    # Extract features from landmarks\n",
    "    # Implement your feature extraction logic here\n",
    "    \n",
    "    # Classify facial expression\n",
    "    # Implement your classification logic here\n",
    "    \n",
    "    return expression_label\n",
    "\n",
    "# Find landmarks for each detected face\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "    \n",
    "    # Classify facial expression\n",
    "    expression_label = classify_expression(landmarks)\n",
    "    \n",
    "    # Draw rectangles around the facial expression regions\n",
    "    # You can modify this part to suit your visualization needs\n",
    "    for point in left_eye_points + right_eye_points + mouth_points:\n",
    "        x = landmarks.part(point).x\n",
    "        y = landmarks.part(point).y\n",
    "        cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the classified facial expression\n",
    "    cv2.putText(image, expression_label, (face.left(), face.bottom()), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"output_img11.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
