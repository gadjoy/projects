{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open shape_predictor_68_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m predictor_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Download path below\u001b[39;00m\n\u001b[1;32m      7\u001b[0m detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[0;32m----> 8\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load an image (replace with your image path or 0 for webcam)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mamta/projects/facial_analysis/server/img11.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to open shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download path below\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path or 0 for webcam)\n",
    "image = cv2.imread(\"/home/mamta/projects/facial_analysis/server/img11.jpg\") \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# Find landmarks for each detected face\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "\n",
    "    # Draw circles on each landmark point\n",
    "    for n in range(0, 68):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"output_image.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mamta/projects/facial_analysis/server/img11.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Load pre-trained model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/mamta/projects/facial_analysis/server/img11.jpg\")\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "# For each detected face, predict emotion\n",
    "for (x, y, w, h) in faces:\n",
    "    # Crop face\n",
    "    face = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Predict emotion\n",
    "    result = DeepFace.analyze(face, actions=['emotion'])\n",
    "\n",
    "    # Extract emotion\n",
    "    emotion = result['dominant_emotion']\n",
    "    \n",
    "    # Draw bounding box around the face\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Put text with emotion label\n",
    "    cv2.putText(image, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open shape_predictor_68_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m emotion_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisgust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHappy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSurprise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[0;32m---> 10\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictor_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load an image (replace with your image path or 0 for webcam)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mamta/projects/facial_analysis/server/img11.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to open shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Pre-trained facial landmark detector\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"  # Download path below\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load an image (replace with your image path or 0 for webcam)\n",
    "image = cv2.imread(\"/home/mamta/projects/facial_analysis/server/img11.jpg\") \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = detector(gray)\n",
    "\n",
    "# For each detected face, predict emotion\n",
    "for face in faces:\n",
    "    # Find landmarks\n",
    "    landmarks = predictor(gray, face)\n",
    "    \n",
    "    # Crop face region for emotion analysis\n",
    "    (x, y, w, h) = (face.left(), face.top(), face.width(), face.height())\n",
    "    face_region = gray[y:y+h, x:x+w]\n",
    "\n",
    "    # Predict emotion\n",
    "    result = DeepFace.analyze(face_region, actions=['emotion'])\n",
    "\n",
    "    # Get dominant emotion\n",
    "    dominant_emotion = max(result['emotion'].items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    # Draw bounding box around the face\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Put text with predicted emotion\n",
    "    cv2.putText(image, dominant_emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite(\"output_image.jpg\", image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
